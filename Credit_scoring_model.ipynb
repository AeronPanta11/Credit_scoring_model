{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsY293WAWjEXWtXipiMpeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AeronPanta11/Credit_scoring_model/blob/main/Credit_scoring_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgBGfKKRjInc",
        "outputId": "164b2ce9-6f69-49cc-cf96-ac8ed2fcbd3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/shebrahimi/financial-distress\n",
            "License(s): other\n",
            "Downloading financial-distress.zip to /content\n",
            "  0% 0.00/815k [00:00<?, ?B/s]\n",
            "100% 815k/815k [00:00<00:00, 37.1MB/s]\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# train_person_1_feats_1 = train_person_1.group_by(\"case_id\").agg(\n",
        "#     pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"),\n",
        "#     (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")\n",
        "# )\n",
        "\n",
        "# # Here num_group1=0 has special meaning, it is the person who applied for the loan.\n",
        "# train_person_1_feats_2 = train_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n",
        "#     pl.col(\"num_group1\") == 0\n",
        "# ).drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetype\"})\n",
        "\n",
        "# # Here we have num_goup1 and num_group2, so we need to aggregate again.\n",
        "# train_credit_bureau_b_2_feats = train_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
        "#     pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"pmts_pmtsoverdue_635A_max\"),\n",
        "#     (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n",
        "# )\n",
        "\n",
        "# # We will process in this examples only A-type and M-type columns, so we need to select them.\n",
        "# selected_static_cols = []\n",
        "# for col in train_static.columns:\n",
        "#     if col[-1] in (\"A\", \"M\"):\n",
        "#         selected_static_cols.append(col)\n",
        "# print(selected_static_cols)\n",
        "\n",
        "# selected_static_cb_cols = []\n",
        "# for col in train_static_cb.columns:\n",
        "#     if col[-1] in (\"A\", \"M\"):\n",
        "#         selected_static_cb_cols.append(col)\n",
        "# print(selected_static_cb_cols)\n",
        "\n",
        "# # Join all tables together.\n",
        "# data = train_basetable.join(\n",
        "#     train_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     train_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     train_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     train_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     train_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
        "# )\n",
        "# Step 1: Aggregate features for train_person_1\n",
        "train_person_1_feats = train_person_1.group_by(\"case_id\").agg(\n",
        "    pl.max(\"mainoccupationinc_384A\").alias(\"mainoccupationinc_384A_max\"),\n",
        "    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\"),\n",
        "    pl.when(pl.col(\"num_group1\") == 0).then(pl.col(\"housetype_905L\")).alias(\"person_housetype\")\n",
        ").filter(pl.col(\"person_housetype\").is_not_null())\n",
        "\n",
        "# Step 2: Aggregate features for train_credit_bureau_b_2\n",
        "train_credit_bureau_feats = train_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
        "    pl.max(\"pmts_pmtsoverdue_635A\").alias(\"pmts_pmtsoverdue_635A_max\"),\n",
        "    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n",
        ")\n",
        "\n",
        "# Step 3: Select relevant static columns from train_static and train_static_cb\n",
        "selected_static_cols = [col for col in train_static.columns if col.endswith((\"A\", \"M\"))]\n",
        "selected_static_cb_cols = [col for col in train_static_cb.columns if col.endswith((\"A\", \"M\"))]\n",
        "\n",
        "# Step 4: Join all tables together\n",
        "data = (\n",
        "    train_basetable\n",
        "    .join(train_static.select([\"case_id\"] + selected_static_cols), how=\"left\", on=\"case_id\")\n",
        "    .join(train_static_cb.select([\"case_id\"] + selected_static_cb_cols), how=\"left\", on=\"case_id\")\n",
        "    .join(train_person_1_feats, how=\"left\", on=\"case_id\")\n",
        "    .join(train_credit_bureau_feats, how=\"left\", on=\"case_id\")\n",
        ")\n",
        "# test_person_1_feats_1 = test_person_1.group_by(\"case_id\").agg(\n",
        "#     pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"),\n",
        "#     (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")\n",
        "# )\n",
        "\n",
        "# test_person_1_feats_2 = test_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n",
        "#     pl.col(\"num_group1\") == 0\n",
        "# ).drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetype\"})\n",
        "\n",
        "# test_credit_bureau_b_2_feats = test_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
        "#     pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"pmts_pmtsoverdue_635A_max\"),\n",
        "#     (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n",
        "# )\n",
        "\n",
        "# data_submission = test_basetable.join(\n",
        "#     test_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     test_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     test_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     test_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
        "# ).join(\n",
        "#     test_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
        "# )\n",
        "# Step 1: Aggregate features for test_person_1\n",
        "test_person_1_feats = test_person_1.group_by(\"case_id\").agg(\n",
        "    pl.max(\"mainoccupationinc_384A\").alias(\"mainoccupationinc_384A_max\"),\n",
        "    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\"),\n",
        "    pl.when(pl.col(\"num_group1\") == 0).then(pl.col(\"housetype_905L\")).alias(\"person_housetype\")\n",
        ").filter(pl.col(\"person_housetype\").is_not_null())\n",
        "\n",
        "# Step 2: Aggregate features for test_credit_bureau_b_2\n",
        "test_credit_bureau_feats = test_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
        "    pl.max(\"pmts_pmtsoverdue_635A\").alias(\"pmts_pmtsoverdue_635A_max\"),\n",
        "    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n",
        ")\n",
        "\n",
        "# Step 3: Join all tables together\n",
        "data_submission = (\n",
        "    test_basetable\n",
        "    .join(test_static.select([\"case_id\"] + selected_static_cols), how=\"left\", on=\"case_id\")\n",
        "    .join(test_static_cb.select([\"case_id\"] + selected_static_cb_cols), how=\"left\", on=\"case_id\")\n",
        "    .join(test_person_1_feats, how=\"left\", on=\"case_id\")\n",
        "    .join(test_credit_bureau_feats, how=\"left\", on=\"case_id\")\n",
        ")\n",
        "# case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
        "# case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
        "# case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
        "\n",
        "# cols_pred = []\n",
        "# for col in data.columns:\n",
        "#     if col[-1].isupper() and col[:-1].islower():\n",
        "#         cols_pred.append(col)\n",
        "\n",
        "# print(cols_pred)\n",
        "\n",
        "# def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n",
        "#     return (\n",
        "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n",
        "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n",
        "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n",
        "#     )\n",
        "\n",
        "# base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
        "# base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
        "# base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
        "\n",
        "# for df in [X_train, X_valid, X_test]:\n",
        "#     df = convert_strings(df)\n",
        "# Step 1: Shuffle and split case_ids\n",
        "case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
        "case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
        "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
        "\n",
        "# Step 2: Identify prediction columns\n",
        "cols_pred = [col for col in data.columns if col[-1].isupper() and col[:-1].islower()]\n",
        "print(cols_pred)\n",
        "\n",
        "# Step 3: Function to convert Polars DataFrame to Pandas\n",
        "def from_polars_to_pandas(case_ids: pl.Series) -> tuple:\n",
        "    filtered_data = data.filter(pl.col(\"case_id\").is_in(case_ids))\n",
        "    return (\n",
        "        filtered_data[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n",
        "        filtered_data[cols_pred].to_pandas(),\n",
        "        filtered_data[\"target\"].to_pandas()\n",
        "    )\n",
        "\n",
        "# Step 4: Create train, validation, and test sets\n",
        "base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
        "base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
        "base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
        "\n",
        "# Step 5: Convert string columns in DataFrames\n",
        "for df in [X_train, X_valid, X_test]:\n",
        "    df = convert_strings(df)  # Ensure this function modifies df in place or return modified df\n",
        "['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A', 'description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n",
        "print(f\"Train: {X_train.shape}\")\n",
        "print(f\"Valid: {X_valid.shape}\")\n",
        "print(f\"Test: {X_test.shape}\")\n",
        "Train: (915995, 48)\n",
        "Valid: (305332, 48)\n",
        "Test: (305332, 48)\n",
        "Training LightGBM\n",
        "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
        "lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
        "\n",
        "params = {\n",
        "    # \"boosting_type\": \"gbdt\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"max_depth\": 100,\n",
        "    \"num_leaves\": 100,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"bagging_fraction\": 0.7,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"min_data_in_leaf\": 30,\n",
        "    \"lambda_l1\": 0.1,\n",
        "    \"lambda_l2\": 0.1,\n",
        "    \"n_estimators\": 1000,\n",
        "    \"verbose\": -1,\n",
        "}\n",
        "gbm = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    valid_sets=lgb_valid,\n",
        "    callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n",
        ")\n",
        "```python\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "dataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"\n",
        "def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    # implement here all desired dtypes for tables\n",
        "    # the following is just an example\n",
        "    for col in df.columns:\n",
        "        # last letter of column name will help you determine the type\n",
        "        if col[-1] in (\"P\", \"A\"):\n",
        "            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype.name in ['object', 'string']:\n",
        "            df[col] = df[col].astype(\"string\").astype('category')\n",
        "            current_categories = df[col].cat.categories\n",
        "            new_categories = current_categories.to_list() + [\"Unknown\"]\n",
        "            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
        "            df[col] = df[col].astype(new_dtype)\n",
        "    return df\n",
        "train_basetable = pl.read_csv(dataPath + \"csv_files/train/train_base.csv\")\n",
        "train_static = pl.concat(\n",
        "    [\n",
        "        pl.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\").pipe(set_table_dtypes),\n",
        "        pl.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\").pipe(set_table_dtypes),\n",
        "    ],\n",
        "    how=\"vertical_relaxed\",\n",
        ")\n",
        "train_static_cb = pl.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\").pipe(set_table_dtypes)\n",
        "train_person_1 = pl.read_csv(dataPath + \"csv_files/train/train_person_1.csv\").pipe(set_table_dtypes)\n",
        "train_credit_bureau_b_2 = pl.read_csv(dataPath + \"csv_files/train/train_credit_bureau_b_2.csv\").pipe(set_table_dtypes)\n",
        "test_basetable = pl.read_csv(dataPath + \"csv_files/test/test_base.csv\")\n",
        "test_static = pl.concat(\n",
        "    [\n",
        "        pl.read_csv(dataPath + \"csv_files/test/test_static_0_0.csv\").pipe(set_table_dtypes),\n",
        "        pl.read_csv(dataPath + \"csv_files/test/test_static_0_1.csv\").pipe(set_table_dtypes),\n",
        "        pl.read_csv(dataPath + \"csv_files/test/test_static_0_2.csv\").pipe(set_table_dtypes),\n",
        "    ],\n",
        "    how=\"vertical_relaxed\",\n",
        ")\n",
        "test_static_cb = pl.read_csv(dataPath + \"csv_files/test/test_static_cb_0.csv\").pipe(set_table_dtypes)\n",
        "test_person_1 = pl.read_csv(dataPath + \"csv_files/test/test_person_1.csv\").pipe(set_table_dtypes)\n",
        "test_credit_bureau_b_2 = pl.read_csv(dataPath + \"csv_files/test/test_credit_bureau_b_2.csv\").pipe(set_table_dtypes)\n",
        "\n",
        "# Step 1: Aggregate features for train_person_1\n",
        "train_person_1_feats = train_person_1.group_by(\"case_id\").agg(\n",
        "    pl.max(\"mainoccupationinc_384A\").alias(\"mainoccupationinc_384A_max\"),\n",
        "    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\"),\n",
        "    pl.when(pl.col(\"num_group1\") == 0).then(pl.col(\"housetype_905L\")).alias(\"person_housetype\")\n",
        ").filter(pl.col(\"person_housetype\").is_not_null())\n",
        "\n",
        "# Step 2: Aggregate features for train_credit_bureau_b_2\n",
        "train_credit_bureau_feats = train_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
        "    pl.max(\"pmts_pmtsoverdue_635A\").alias(\"pmts_pmtsoverdue_635A_max\"),\n",
        "    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n",
        ")\n",
        "\n",
        "# Step 3: Select relevant static columns from train_static and train_static_cb\n",
        "selected_static_cols = [col for col in train_static.columns if col.endswith((\"A\", \"M\"))]\n",
        "selected_static_cb_cols = [col for col in train_static_cb.columns if col.endswith((\"A\", \"M\"))]\n",
        "\n",
        "# Step 4: Join all tables together\n",
        "data = (\n",
        "    train_basetable\n",
        "    .join(train_static.select([\"case_id\"] + selected_static_cols), how=\"left\", on=\"case_id\")\n",
        "    .join(train_static_cb.select([\"case_id\"] + selected_static_cb_cols), how=\"left\", on=\"case_id\")\n",
        "    .join(train_person_1_feats, how=\"left\", on=\"case_id\")\n",
        "    .join(train_credit_bureau_feats, how=\"left\", on=\"case_id\")\n",
        ")\n",
        "\n",
        "# Step 1: Aggregate features for test_person_1\n",
        "test_person_1_feats = test_person_1.group_by(\"case_id\").agg(\n",
        "    pl.max(\"mainoccupationinc_384A\").alias(\"mainoccupationinc_384A_max\"),\n",
        "    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\"),\n",
        "    pl.when(pl.col(\"num_group1\") == 0).then(pl.col(\"housetype_905L\")).alias(\"person_housetype\")\n",
        ").filter(pl.col(\"person_housetype\").is_not_null())\n",
        "\n",
        "# Step 2: Aggregate features for test_credit_bureau_b_2\n",
        "test_credit_bureau_feats = test_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
        "    pl.max(\"pmts_pmtsoverdue_635A\").alias(\"pmts_pmtsoverdue_635A_max\"),\n",
        "    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n",
        ")\n",
        "\n",
        "# Step 3: Join all tables together\n",
        "data_submission = (\n",
        "    test_basetable\n",
        "    .join(test_static.select([\"case_id\"] + selected_static_cols), how=\"left\", on=\"case_id\")\n",
        "    .join(test_static_cb.select([\"case_id\"] + selected_static_cb_cols), how=\"left\", on=\"case_id\")\n",
        "    .join(test_person_1_feats, how=\"left\", on=\"case_id\")\n",
        "    .join(test_credit_bureau_feats, how=\"left\", on=\"case_id\")\n",
        ")\n",
        "# Step 1: Shuffle and split case_ids\n",
        "case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
        "case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
        "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
        "\n",
        "# Step 2: Identify prediction columns\n",
        "cols_pred = [col for col in data.columns if col[-1].isupper() and col[:-1].islower()]\n",
        "print(cols_pred)\n",
        "\n",
        "# Step 3: Function to convert Polars DataFrame to Pandas\n",
        "def from_polars_to_pandas(case_ids: pl.Series) -> tuple:\n",
        "    filtered_data = data.filter(pl.col(\"case_id\").is_in(case_ids))\n",
        "    return (\n",
        "        filtered_data[[\"case_id\", \"WEEK_NUM\", \"target\"]].\n",
        "[150]\tvalid_0's auc: 0.729958\n",
        "[200]\tvalid_0's auc: 0.734722\n",
        "[250]\tvalid_0's auc: 0.737777\n",
        "[300]\tvalid_0's auc: 0.740642\n",
        "[350]\tvalid_0's auc: 0.742941\n",
        "[400]\tvalid_0's auc: 0.744654\n",
        "[450]\tvalid_0's auc: 0.746325\n",
        "[500]\tvalid_0's auc: 0.747905\n",
        "[550]\tvalid_0's auc: 0.748905\n",
        "[600]\tvalid_0's auc: 0.749693\n",
        "[650]\tvalid_0's auc: 0.75033\n",
        "[700]\tvalid_0's auc: 0.75114\n",
        "[750]\tvalid_0's auc: 0.751672\n",
        "[800]\tvalid_0's auc: 0.752056\n",
        "[850]\tvalid_0's auc: 0.752576\n",
        "[900]\tvalid_0's auc: 0.752945\n",
        "[950]\tvalid_0's auc: 0.753269\n",
        "[1000]\tvalid_0's auc: 0.753553\n",
        "Did not meet early stopping. Best iteration is:\n",
        "[998]\tvalid_0's auc: 0.753555\n",
        "for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n",
        "    y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n",
        "    base[\"score\"] = y_pred\n",
        "\n",
        "print(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}')\n",
        "print(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}')\n",
        "print(f'The AUC score on the test set is: {roc_auc_score(base_test[\"target\"], base_test[\"score\"])}')\n",
        "The AUC score on the train set is: 0.8059429843256872\n",
        "The AUC score on the valid set is: 0.7535548699023691\n",
        "The AUC score on the test set is: 0.7509128032532776\n",
        "def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
        "    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
        "        .sort_values(\"WEEK_NUM\")\\\n",
        "        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
        "        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n",
        "\n",
        "    x = np.arange(len(gini_in_time))\n",
        "    y = gini_in_time\n",
        "    a, b = np.polyfit(x, y, 1)\n",
        "    y_hat = a*x + b\n",
        "    residuals = y - y_hat\n",
        "    res_std = np.std(residuals)\n",
        "    avg_gini = np.mean(gini_in_time)\n",
        "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
        "\n",
        "stability_score_train = gini_stability(base_train)\n",
        "stability_score_valid = gini_stability(base_valid)\n",
        "stability_score_test = gini_stability(base_test)\n",
        "\n",
        "print(f'The stability score on the train set is: {stability_score_train}')\n",
        "print(f'The stability score on the valid set is: {stability_score_valid}')\n",
        "print(f'The stability score on the test set is: {stability_score_test}')\n",
        "The stability score on the train set is: 0.5892901842986822\n",
        "The stability score on the valid set is: 0.4776052594607597\n",
        "The stability score on the test set is: 0.4616869330704408\n",
        "X_submission = data_submission[cols_pred].to_pandas()\n",
        "X_submission = convert_strings(X_submission)\n",
        "categorical_cols = X_train.select_dtypes(include=['category']).columns\n",
        "\n",
        "for col in categorical_cols:\n",
        "    train_categories = set(X_train[col].cat.categories)\n",
        "    submission_categories = set(X_submission[col].cat.categories)\n",
        "    new_categories = submission_categories - train_categories\n",
        "    X_submission.loc[X_submission[col].isin(new_categories), col] = \"Unknown\"\n",
        "    new_dtype = pd.CategoricalDtype(categories=train_categories, ordered=True)\n",
        "    X_train[col] = X_train[col].astype(new_dtype)\n",
        "    X_submission[col] = X_submission[col].astype(new_dtype)\n",
        "\n",
        "y_submission_pred = gbm.predict(X_submission, num_iteration=gbm.best_iteration)\n",
        "submission = pd.DataFrame({\n",
        "    \"case_id\": data_submission[\"case_id\"].to_numpy(),\n",
        "    \"score\": y_submission_pred\n",
        "}).set_index('case_id')\n",
        "submission.to_csv(\"./submission.csv\")\n",
        "submission\n",
        "score\n",
        "case_id\n",
        "57543\t0.008982\n",
        "57549\t0.041941\n",
        "57551\t0.006793\n",
        "57552\t0.013093\n",
        "57569\t0.106355\n",
        "57630\t0.009155\n",
        "57631\t0.049733\n",
        "57632\t-0.002981\n",
        "57633\t0.056199\n",
        "57634\t0.005745\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "lmTA-Ih9scIn",
        "outputId": "2db7f18f-fdaf-4d79-ac26-f7db9385d22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 361) (<ipython-input-6-4f4061f85720>, line 361)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-4f4061f85720>\"\u001b[0;36m, line \u001b[0;32m361\u001b[0m\n\u001b[0;31m    [150]\tvalid_0's auc: 0.729958\u001b[0m\n\u001b[0m         \t       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 361)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHHe0GS1anNP",
        "outputId": "4b1a217f-dce9-432d-c895-f2a9e52a1eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"\n",
        "def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    # implement here all desired dtypes for tables\n",
        "    # the following is just an example\n",
        "    for col in df.columns:\n",
        "        # last letter of column name will help you determine the type\n",
        "        if col[-1] in (\"P\", \"A\"):\n",
        "            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "fbgfUSTeaojj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype.name in ['object', 'string']:\n",
        "            df[col] = df[col].astype(\"string\").astype('category')\n",
        "            current_categories = df[col].cat.categories\n",
        "            new_categories = current_categories.to_list() + [\"Unknown\"]\n",
        "            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
        "            df[col] = df[col].astype(new_dtype)\n",
        "    return df"
      ],
      "metadata": {
        "id": "sgHjjLnFaxXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dd65c__Ra8mh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}